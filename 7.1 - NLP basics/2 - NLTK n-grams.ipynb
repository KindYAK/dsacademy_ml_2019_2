{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving N-Grams from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"The quick brown fox jumps over the lazy dog.\"\n",
    "s = s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Z]+\")\n",
    "s_tokenized = tokenizer.tokenize(s)\n",
    "s_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('_', '_', '_', 't'),\n",
       "  ('_', '_', 't', 'h'),\n",
       "  ('_', 't', 'h', 'e'),\n",
       "  ('t', 'h', 'e', '_'),\n",
       "  ('h', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 'q'),\n",
       "  ('_', '_', 'q', 'u'),\n",
       "  ('_', 'q', 'u', 'i'),\n",
       "  ('q', 'u', 'i', 'c'),\n",
       "  ('u', 'i', 'c', 'k'),\n",
       "  ('i', 'c', 'k', '_'),\n",
       "  ('c', 'k', '_', '_'),\n",
       "  ('k', '_', '_', '_')],\n",
       " [('_', '_', '_', 'b'),\n",
       "  ('_', '_', 'b', 'r'),\n",
       "  ('_', 'b', 'r', 'o'),\n",
       "  ('b', 'r', 'o', 'w'),\n",
       "  ('r', 'o', 'w', 'n'),\n",
       "  ('o', 'w', 'n', '_'),\n",
       "  ('w', 'n', '_', '_'),\n",
       "  ('n', '_', '_', '_')],\n",
       " [('_', '_', '_', 'f'),\n",
       "  ('_', '_', 'f', 'o'),\n",
       "  ('_', 'f', 'o', 'x'),\n",
       "  ('f', 'o', 'x', '_'),\n",
       "  ('o', 'x', '_', '_'),\n",
       "  ('x', '_', '_', '_')],\n",
       " [('_', '_', '_', 'j'),\n",
       "  ('_', '_', 'j', 'u'),\n",
       "  ('_', 'j', 'u', 'm'),\n",
       "  ('j', 'u', 'm', 'p'),\n",
       "  ('u', 'm', 'p', 's'),\n",
       "  ('m', 'p', 's', '_'),\n",
       "  ('p', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')],\n",
       " [('_', '_', '_', 'o'),\n",
       "  ('_', '_', 'o', 'v'),\n",
       "  ('_', 'o', 'v', 'e'),\n",
       "  ('o', 'v', 'e', 'r'),\n",
       "  ('v', 'e', 'r', '_'),\n",
       "  ('e', 'r', '_', '_'),\n",
       "  ('r', '_', '_', '_')],\n",
       " [('_', '_', '_', 't'),\n",
       "  ('_', '_', 't', 'h'),\n",
       "  ('_', 't', 'h', 'e'),\n",
       "  ('t', 'h', 'e', '_'),\n",
       "  ('h', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 'l'),\n",
       "  ('_', '_', 'l', 'a'),\n",
       "  ('_', 'l', 'a', 'z'),\n",
       "  ('l', 'a', 'z', 'y'),\n",
       "  ('a', 'z', 'y', '_'),\n",
       "  ('z', 'y', '_', '_'),\n",
       "  ('y', '_', '_', '_')],\n",
       " [('_', '_', '_', 'd'),\n",
       "  ('_', '_', 'd', 'o'),\n",
       "  ('_', 'd', 'o', 'g'),\n",
       "  ('d', 'o', 'g', '_'),\n",
       "  ('o', 'g', '_', '_'),\n",
       "  ('g', '_', '_', '_')]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "generated_4grams = []\n",
    "\n",
    "for word in s_tokenized:\n",
    "    generated_4grams.append(list(ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_'))) # n = 4.\n",
    "generated_4grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `generated_4grams` needs flattening since it's supposed to be a list of 4-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', '_', '_', 't'),\n",
       " ('_', '_', 't', 'h'),\n",
       " ('_', 't', 'h', 'e'),\n",
       " ('t', 'h', 'e', '_'),\n",
       " ('h', 'e', '_', '_'),\n",
       " ('e', '_', '_', '_'),\n",
       " ('_', '_', '_', 'q'),\n",
       " ('_', '_', 'q', 'u'),\n",
       " ('_', 'q', 'u', 'i'),\n",
       " ('q', 'u', 'i', 'c')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_4grams = [word for sublist in generated_4grams for word in sublist]\n",
    "generated_4grams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtaining n-grams (n = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['___t',\n",
       " '__th',\n",
       " '_the',\n",
       " 'the_',\n",
       " 'he__',\n",
       " 'e___',\n",
       " '___q',\n",
       " '__qu',\n",
       " '_qui',\n",
       " 'quic',\n",
       " 'uick',\n",
       " 'ick_',\n",
       " 'ck__',\n",
       " 'k___',\n",
       " '___b',\n",
       " '__br',\n",
       " '_bro',\n",
       " 'brow',\n",
       " 'rown',\n",
       " 'own_',\n",
       " 'wn__',\n",
       " 'n___',\n",
       " '___f',\n",
       " '__fo',\n",
       " '_fox',\n",
       " 'fox_',\n",
       " 'ox__',\n",
       " 'x___',\n",
       " '___j',\n",
       " '__ju',\n",
       " '_jum',\n",
       " 'jump',\n",
       " 'umps',\n",
       " 'mps_',\n",
       " 'ps__',\n",
       " 's___',\n",
       " '___o',\n",
       " '__ov',\n",
       " '_ove',\n",
       " 'over',\n",
       " 'ver_',\n",
       " 'er__',\n",
       " 'r___',\n",
       " '___t',\n",
       " '__th',\n",
       " '_the',\n",
       " 'the_',\n",
       " 'he__',\n",
       " 'e___',\n",
       " '___l',\n",
       " '__la',\n",
       " '_laz',\n",
       " 'lazy',\n",
       " 'azy_',\n",
       " 'zy__',\n",
       " 'y___',\n",
       " '___d',\n",
       " '__do',\n",
       " '_dog',\n",
       " 'dog_',\n",
       " 'og__',\n",
       " 'g___']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_list_4grams = generated_4grams\n",
    "for idx, val in enumerate(generated_4grams):\n",
    "    ng_list_4grams[idx] = ''.join(val)\n",
    "ng_list_4grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sorting n-grams by frequency (n = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('___t', 2),\n",
       " ('__th', 2),\n",
       " ('_the', 2),\n",
       " ('the_', 2),\n",
       " ('he__', 2),\n",
       " ('e___', 2),\n",
       " ('___q', 1),\n",
       " ('__qu', 1),\n",
       " ('_qui', 1),\n",
       " ('quic', 1),\n",
       " ('uick', 1),\n",
       " ('ick_', 1),\n",
       " ('ck__', 1),\n",
       " ('k___', 1),\n",
       " ('___b', 1),\n",
       " ('__br', 1),\n",
       " ('_bro', 1),\n",
       " ('brow', 1),\n",
       " ('rown', 1),\n",
       " ('own_', 1),\n",
       " ('wn__', 1),\n",
       " ('n___', 1),\n",
       " ('___f', 1),\n",
       " ('__fo', 1),\n",
       " ('_fox', 1),\n",
       " ('fox_', 1),\n",
       " ('ox__', 1),\n",
       " ('x___', 1),\n",
       " ('___j', 1),\n",
       " ('__ju', 1),\n",
       " ('_jum', 1),\n",
       " ('jump', 1),\n",
       " ('umps', 1),\n",
       " ('mps_', 1),\n",
       " ('ps__', 1),\n",
       " ('s___', 1),\n",
       " ('___o', 1),\n",
       " ('__ov', 1),\n",
       " ('_ove', 1),\n",
       " ('over', 1),\n",
       " ('ver_', 1),\n",
       " ('er__', 1),\n",
       " ('r___', 1),\n",
       " ('___l', 1),\n",
       " ('__la', 1),\n",
       " ('_laz', 1),\n",
       " ('lazy', 1),\n",
       " ('azy_', 1),\n",
       " ('zy__', 1),\n",
       " ('y___', 1),\n",
       " ('___d', 1),\n",
       " ('__do', 1),\n",
       " ('_dog', 1),\n",
       " ('dog_', 1),\n",
       " ('og__', 1),\n",
       " ('g___', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_4grams = {}\n",
    "\n",
    "for ngram in ng_list_4grams:\n",
    "    if ngram not in freq_4grams:\n",
    "        freq_4grams.update({ngram: 1})\n",
    "    else:\n",
    "        ngram_occurrences = freq_4grams[ngram]\n",
    "        freq_4grams.update({ngram: ngram_occurrences + 1})\n",
    "        \n",
    "from operator import itemgetter # The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python. For example, operator.add(x, y) is equivalent to the expression x + y.\n",
    "\n",
    "freq_4grams_sorted = sorted(freq_4grams.items(), key=itemgetter(1), reverse=True)[0:300] # We only keep the 300 most popular n-grams. This was suggested in the original paper written about n-grams.\n",
    "freq_4grams_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Obtaining n-grams for multiple values of n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get n-grams for n = 1, 2, 3 and 4 we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox jumps over the lazy dog'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import everygrams\n",
    "\n",
    "s_clean = ' '.join(s_tokenized) # For the code below we need the raw sentence as opposed to the tokens.\n",
    "s_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 'h',\n",
       " 'e',\n",
       " 'q',\n",
       " 'u',\n",
       " 'i',\n",
       " 'c',\n",
       " 'k',\n",
       " 'b',\n",
       " 'r',\n",
       " 'o',\n",
       " 'w',\n",
       " 'n',\n",
       " 'f',\n",
       " 'o',\n",
       " 'x',\n",
       " 'j',\n",
       " 'u',\n",
       " 'm',\n",
       " 'p',\n",
       " 's',\n",
       " 'o',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'l',\n",
       " 'a',\n",
       " 'z',\n",
       " 'y',\n",
       " 'd',\n",
       " 'o',\n",
       " 'g',\n",
       " 'th',\n",
       " 'he',\n",
       " 'e_',\n",
       " '_q',\n",
       " 'qu',\n",
       " 'ui',\n",
       " 'ic',\n",
       " 'ck',\n",
       " 'k_',\n",
       " '_b',\n",
       " 'br',\n",
       " 'ro',\n",
       " 'ow',\n",
       " 'wn',\n",
       " 'n_',\n",
       " '_f',\n",
       " 'fo',\n",
       " 'ox',\n",
       " 'x_',\n",
       " '_j',\n",
       " 'ju',\n",
       " 'um',\n",
       " 'mp',\n",
       " 'ps',\n",
       " 's_',\n",
       " '_o',\n",
       " 'ov',\n",
       " 've',\n",
       " 'er',\n",
       " 'r_',\n",
       " '_t',\n",
       " 'th',\n",
       " 'he',\n",
       " 'e_',\n",
       " '_l',\n",
       " 'la',\n",
       " 'az',\n",
       " 'zy',\n",
       " 'y_',\n",
       " '_d',\n",
       " 'do',\n",
       " 'og',\n",
       " 'the',\n",
       " 'he_',\n",
       " '_qu',\n",
       " 'qui',\n",
       " 'uic',\n",
       " 'ick',\n",
       " 'ck_',\n",
       " '_br',\n",
       " 'bro',\n",
       " 'row',\n",
       " 'own',\n",
       " 'wn_',\n",
       " '_fo',\n",
       " 'fox',\n",
       " 'ox_',\n",
       " '_ju',\n",
       " 'jum',\n",
       " 'ump',\n",
       " 'mps',\n",
       " 'ps_',\n",
       " '_ov',\n",
       " 'ove',\n",
       " 'ver',\n",
       " 'er_',\n",
       " '_th',\n",
       " 'the',\n",
       " 'he_',\n",
       " '_la',\n",
       " 'laz',\n",
       " 'azy',\n",
       " 'zy_',\n",
       " '_do',\n",
       " 'dog',\n",
       " 'the_',\n",
       " '_qui',\n",
       " 'quic',\n",
       " 'uick',\n",
       " 'ick_',\n",
       " '_bro',\n",
       " 'brow',\n",
       " 'rown',\n",
       " 'own_',\n",
       " '_fox',\n",
       " 'fox_',\n",
       " '_jum',\n",
       " 'jump',\n",
       " 'umps',\n",
       " 'mps_',\n",
       " '_ove',\n",
       " 'over',\n",
       " 'ver_',\n",
       " '_the',\n",
       " 'the_',\n",
       " '_laz',\n",
       " 'lazy',\n",
       " 'azy_',\n",
       " '_dog']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngram_extractor(sent):\n",
    "    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) \n",
    "            if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n",
    "\n",
    "ngram_extractor(s_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
