{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Explanations Method (CEM) applied to Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Contrastive Explanation Method (CEM) can generate black box model explanations in terms of pertinent positives (PP) and pertinent negatives (PN). For PP, it finds what should be minimally and sufficiently present (e.g. important pixels in an image) to justify its classification. PN on the other hand identify what should be minimally and necessarily absent from the explained instance in order to maintain the original prediction.\n",
    "\n",
    "The original paper where the algorithm is based on can be found on [arXiv](https://arxiv.org/pdf/1802.07623.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "import tensorflow as tf\n",
    "from alibi.explainers import CEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()\n",
    "feature_names = dataset.feature_names\n",
    "class_names = list(dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data = (dataset.data - dataset.data.mean(axis=0)) / dataset.data.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 145\n",
    "x_train,y_train = dataset.data[:idx,:], dataset.target[:idx]\n",
    "x_test, y_test = dataset.data[idx+1:,:], dataset.target[idx+1:]\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model():\n",
    "    x_in = Input(shape=(4,))\n",
    "    x_out = Dense(3, activation='softmax')(x_in)\n",
    "    lr = Model(inputs=x_in, outputs=x_out)\n",
    "    lr.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lr = lr_model()\n",
    "lr.summary()\n",
    "lr.fit(x_train, y_train, batch_size=128, epochs=500, verbose=0)\n",
    "lr.save('iris_lr.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate contrastive explanation with pertinent negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on instance to be explained: virginica\n",
      "Prediction probabilities for each class on the instance: [[0.00687267 0.4659831  0.52714425]]\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "X = x_test[idx].reshape((1,) + x_test[idx].shape)\n",
    "print('Prediction on instance to be explained: {}'.format(class_names[np.argmax(lr.predict(X))]))\n",
    "print('Prediction probabilities for each class on the instance: {}'.format(lr.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEM parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'PN'  # 'PN' (pertinent negative) or 'PP' (pertinent positive)\n",
    "shape = (1,) + x_train.shape[1:]  # instance shape\n",
    "kappa = .2  # minimum difference needed between the prediction probability for the perturbed instance on the\n",
    "            # class predicted by the original instance and the max probability on the other classes \n",
    "            # in order for the first loss term to be minimized\n",
    "beta = .1  # weight of the L1 loss term\n",
    "c_init = 10.  # initial weight c of the loss term encouraging to predict a different class (PN) or \n",
    "              # the same class (PP) for the perturbed instance compared to the original instance to be explained\n",
    "c_steps = 10  # nb of updates for c\n",
    "max_iterations = 1000  # nb of iterations per value of c\n",
    "feature_range = (x_train.min(axis=0).reshape(shape)-.1,  # feature range for the perturbed instance\n",
    "                 x_train.max(axis=0).reshape(shape)+.1)  # can be either a float or array of shape (1xfeatures)\n",
    "clip = (-1000.,1000.)  # gradient clipping\n",
    "lr_init = 1e-2  # initial learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pertinent negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init session before model definition\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# define model\n",
    "lr = load_model('iris_lr.h5')\n",
    "\n",
    "# initialize CEM explainer and explain instance\n",
    "cem = CEM(sess, lr, mode, shape, kappa=kappa, beta=beta, feature_range=feature_range, \n",
    "          max_iterations=max_iterations, c_init=c_init, c_steps=c_steps, \n",
    "          learning_rate_init=lr_init, clip=clip)\n",
    "cem.fit(x_train, no_info_type='median')  # we need to define what feature values contain the least\n",
    "                                         # info wrt predictions\n",
    "                                         # here we will naively assume that the feature-wise median\n",
    "                                         # contains no info; domain knowledge helps!\n",
    "explanation = cem.explain(X, verbose=False)\n",
    "sess.close()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original instance: [[ 0.55333328 -1.28296331  0.70592084  0.92230284]]\n",
      "Predicted class: virginica\n"
     ]
    }
   ],
   "source": [
    "print('Original instance: {}'.format(explanation['X']))\n",
    "print('Predicted class: {}'.format(class_names[explanation['X_pred']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pertinent negative: [[ 0.5533333  -2.1709094   0.7059208   0.92230284]]\n",
      "Predicted class: versicolor\n"
     ]
    }
   ],
   "source": [
    "print('Pertinent negative: {}'.format(explanation[mode]))\n",
    "print('Predicted class: {}'.format(class_names[explanation[mode + '_pred']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store explanation to plot later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl = {}\n",
    "expl['PN'] = explanation[mode]\n",
    "expl['PN_pred'] = explanation[mode + '_pred']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate pertinent positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'PP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pertinent positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init session before model definition\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# define model\n",
    "lr = load_model('iris_lr.h5')\n",
    "\n",
    "# initialize CEM explainer and explain instance\n",
    "cem = CEM(sess, lr, mode, shape, kappa=kappa, beta=beta, feature_range=feature_range, \n",
    "          max_iterations=max_iterations, c_init=c_init, c_steps=c_steps, \n",
    "          learning_rate_init=lr_init, clip=clip)\n",
    "cem.fit(x_train, no_info_type='median')\n",
    "explanation = cem.explain(X, verbose=False)\n",
    "sess.close()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pertinent positive: {}'.format(explanation[mode]))\n",
    "print('Predicted class: {}'.format(class_names[explanation[mode + '_pred']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl['PP'] = explanation[mode]\n",
    "expl['PP_pred'] = explanation[mode + '_pred']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize PN and PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the generated explanations to check if the perturbed instances make sense.\n",
    "\n",
    "Create dataframe from standardized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df['species'] = np.array([dataset.target_names[i] for i in dataset.target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlight explained instance and add pertinent negative and positive to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn = pd.DataFrame(expl['PN'], columns=dataset.feature_names)\n",
    "pn['species'] = 'PN_' + class_names[expl['PN_pred']]\n",
    "pp = pd.DataFrame(expl['PP'], columns=dataset.feature_names)\n",
    "pp['species'] = 'PP_' + class_names[expl['PP_pred']]\n",
    "orig_inst = pd.DataFrame(explanation['X'], columns=dataset.feature_names)\n",
    "orig_inst['species'] = 'orig_' + class_names[explanation['X_pred']]\n",
    "df = df.append([pn, pp, orig_inst], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair plots between the features show that the pertinent negative is pushed from the original instance (versicolor) into the virginica distribution while the pertinent positive moved away from the virginica distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.pairplot(df, hue='species', diag_kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use numerical gradients in CEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not have access to the Keras or TensorFlow model weights, we can use numerical gradients for the first term in the loss function that needs to be minimized (eq. 1 and 4 in the [paper](https://arxiv.org/pdf/1802.07623.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEM parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'PN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If numerical gradients are used to compute:\n",
    "\n",
    "\\begin{equation*} \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial p} \\ast \\frac{\\partial p}{\\partial x} \\end{equation*}\n",
    "\n",
    "with L = loss function; p = predict function and x the parameter to optimize, then the tuple *eps* can be used to define the perturbation used to compute the derivatives. *eps[0]* is used to calculate the first partial derivative term and *eps[1]* is used for the second term. *eps[0]* and *eps[1]* can be a combination of float values or numpy arrays. For *eps[0]*, the array dimension should be *(1 x nb of prediction categories)* and for *eps[1]* it should be *(1 x nb of features)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps0 = np.array([[1e-2, 1e-2, 1e-2]])  # 3 prediction categories, equivalent to 1e-2\n",
    "eps1 = np.array([[1e-2, 1e-2, 1e-2, 1e-2]])  # 4 features, also equivalent to 1e-2\n",
    "eps = (eps0, eps1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complex models with a high number of parameters and a high dimensional feature space (e.g. Inception on ImageNet), evaluating numerical gradients can be expensive as they involve multiple prediction calls for each perturbed instance. The *update_num_grad* parameter allows you to set a batch size on which to evaluate the numerical gradients, drastically reducing the number of prediction calls required.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_num_grad = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pertinent negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init session before model definition\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# define model\n",
    "lr = load_model('iris_lr.h5')\n",
    "predict_fn = lambda x: lr.predict(x)  # only pass the predict fn which takes numpy arrays to CEM\n",
    "                                      # explainer can no longer minimize wrt model weights\n",
    "\n",
    "# initialize CEM explainer and explain instance\n",
    "cem = CEM(sess, predict_fn, mode, shape, kappa=kappa, beta=beta, \n",
    "          feature_range=feature_range, max_iterations=max_iterations, \n",
    "          eps=eps, c_init=c_init, c_steps=c_steps, learning_rate_init=lr_init, \n",
    "          clip=clip, update_num_grad=update_num_grad)\n",
    "cem.fit(x_train, no_info_type='median')\n",
    "explanation = cem.explain(X, verbose=False)\n",
    "sess.close()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original instance: {}'.format(explanation['X']))\n",
    "print('Predicted class: {}'.format(class_names[explanation['X_pred']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pertinent negative: {}'.format(explanation[mode]))\n",
    "print('Predicted class: {}'.format(class_names[explanation[mode + '_pred']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('iris_lr.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
